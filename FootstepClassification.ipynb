{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/975125089qq/Simple_game--Pygame/blob/main/FootstepClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCDfaHAyHTQL"
      },
      "source": [
        "# 1. Import and Install Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrXShlnFHTQN"
      },
      "source": [
        "## 1.1 Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af0hMjVJIpzT",
        "outputId": "be88058b-745d-4d26-ca1a-132a6d4c811f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVeth5LuHTQN",
        "outputId": "b1d7b043-5a2e-461b-ec81-14288f292828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-io\n",
            "  Downloading tensorflow_io-0.30.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.30.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-io) (0.30.0)\n",
            "Installing collected packages: tensorflow-io\n",
            "Successfully installed tensorflow-io-0.30.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow tensorflow-gpu  matplotlib\n",
        "!pip install tensorflow-io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh7boHisHTQO"
      },
      "source": [
        "## 1.2 Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-ron_yKxHTQO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf \n",
        "import tensorflow_io as tfio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG3dGUSIHTQO"
      },
      "source": [
        "# 2. Build Data Loading Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s-VqzcUHTQP"
      },
      "source": [
        "## 2.2 Build Dataloading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wHkcyGI6HTQP"
      },
      "outputs": [],
      "source": [
        "def load_wav_16k_mono(filename):\n",
        "    # Load encoded wav file\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    # Decode wav (tensors by channels) \n",
        "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
        "    # Removes trailing axis\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    # Goes from 44100Hz to 16000hz - amplitude of the audio signal\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN8f4DRwHTQQ"
      },
      "source": [
        "# 3. Create Tensorflow Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV-GJGFYHTQQ"
      },
      "source": [
        "## 3.1 Define Paths to Positive and Negative Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lz6xRcyRkPQl"
      },
      "outputs": [],
      "source": [
        "def return_slices(FILEPATH):\n",
        "    wav = load_mp3_16k_mono(FILEPATH)\n",
        "    audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=11200, sequence_stride=11200, batch_size=1)\n",
        "    return audio_slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OpxS2cPsHTQQ"
      },
      "outputs": [],
      "source": [
        "#森田\n",
        "S01 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S01')\n",
        "S02 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S02')\n",
        "S03 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S03')\n",
        "S04 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S04')\n",
        "S05 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S05')\n",
        "S06 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S06')\n",
        "S07 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S07')\n",
        "S08 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S08')\n",
        "S09 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S09')\n",
        "S10 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S10')\n",
        "S11 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S11')\n",
        "S12 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S12')\n",
        "S13 = os.path.join('/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw', 'S13')\n",
        "\n",
        "SS = [S01, S02, S03, S04, S05, S06, S07, S08, S09, S10, S11, S12, S13]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5HCz6GWHTQQ"
      },
      "source": [
        "## 3.2 Create Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks9RtIahW6J0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe3b477-1a26-4d52-d023-b3fbb62d397d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7f5c90f9b670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7f5c90f9b9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "#森田\n",
        "import os\n",
        "\n",
        "s01 = []\n",
        "s02 = []\n",
        "s03 = []\n",
        "s04 = []\n",
        "s05 = []\n",
        "s06 = []\n",
        "s07 = []\n",
        "s08 = []\n",
        "s09 = []\n",
        "s10 = []\n",
        "s11 = []\n",
        "s12 = []\n",
        "s13 = []\n",
        "\n",
        "ss = [s01, s02, s03, s04, s05, s06, s07, s08, s09, s10, s11, s12, s13]\n",
        "\n",
        "for n in range(13):\n",
        "    \n",
        "  for i in os.listdir(SS[n]):\n",
        "    # CALCULATE NUM\n",
        "    wav = load_wav_16k_mono(SS[n] + '/' + i)\n",
        "    wav = wav[:int(len(wav) * 0.7)]\n",
        "\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!短縮版！！！！！！！！！！！！！！\n",
        "    NUM = int(len(wav) / (0.75 * 16000))\n",
        "\n",
        "    for j in range(NUM):\n",
        "      ss[n].append(os.path.join(SS[n],i+str(j)))\n",
        "\n",
        "  ss[n] = tf.data.Dataset.from_tensor_slices(ss[n])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Eo1AbMHTQR"
      },
      "source": [
        "## 3.3 Add labels and Combine Positive and Negative Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8pG6n8BHTQR"
      },
      "outputs": [],
      "source": [
        "#森田\n",
        "# data = tf.data.Dataset()\n",
        "sis = []\n",
        "for i in range(13):\n",
        "\n",
        "  indices = i\n",
        "  depth = 13    \n",
        "\n",
        "  sis.append( tf.data.Dataset.zip((ss[i], tf.data.Dataset.from_tensor_slices(([tf.one_hot(indices, depth)]*len(ss[i]))))) )\n",
        "\n",
        "for i in range(len(sis)-1):\n",
        "  sis[0] = sis[0].concatenate(sis[i+1])\n",
        "data = sis[0]\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "print(data.as_numpy_iterator().next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ8BH2UqHTQR"
      },
      "source": [
        "# 4. Determine Average Length of a Capuchin Call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCfVrKaCHTQS"
      },
      "source": [
        "# 5. Build Preprocessing Function to Convert to Spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQMNJIpQHTQS"
      },
      "source": [
        "## 5.1 Build Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctO2uPLvHTQS"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# def my_numpy_func(x):\n",
        "#   # x will be a numpy array with the contents of the input to the\n",
        "#   # tf.function\n",
        "#   return np.sinh(x)\n",
        "# @tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
        "# def tf_function(input):\n",
        "#   y = tf.numpy_function(my_numpy_func, [input], tf.float32)\n",
        "#   return y * y\n",
        "# tf_function(tf.constant(1.))\n",
        "\n",
        "\n",
        "def preprocess(file_path, label):\n",
        "    num = \"\"\n",
        "    for i in reversed(file_path):\n",
        "      if 48<=i<=57:\n",
        "        num = chr(i) + num\n",
        "      else:\n",
        "        break\n",
        "    wav = load_wav_16k_mono(file_path[:-len(num)])\n",
        "\n",
        "    num = int(num)\n",
        "    print(num)\n",
        "    \n",
        "\n",
        "    ###\n",
        "    wav_clip = wav[12000*num:12000+12000*num]\n",
        "    spectrogram = tf.signal.stft(wav_clip, frame_length=320, frame_step=32)\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "    return spectrogram, label\n",
        "\n",
        "    # # 　分けないと問題ないですが、\n",
        "    # spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
        "    # spectrogram = tf.abs(spectrogram)\n",
        "    # spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "    # return spectrogram, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SntPO4EEHTQS"
      },
      "source": [
        "# 6. Create Training and Testing Partitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv1rwFNwHTQS"
      },
      "source": [
        "## 6.1 Create a Tensorflow Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjntJZm3HTQT"
      },
      "outputs": [],
      "source": [
        "# positives = tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.one_hot(indices=[0]*len(pos),depth=2,on_value=1.0,off_value=0,axis=-1,dtype=tf.float32))))\n",
        "# negatives = tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.one_hot(indices=[1]*len(neg),depth=2,on_value=1.0,off_value=0,axis=-1,dtype=tf.float32))))\n",
        "# # positives = tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.ones(len(pos)))))\n",
        "# # negatives = tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.zeros(len(neg)))))\n",
        "# data = positives.concatenate(negatives)\n",
        "\n",
        "import numpy as np\n",
        "data_numpy = []\n",
        "for i in data:\n",
        "  data_numpy.append([i[0].numpy(), i[1].numpy()])\n",
        "\n",
        "data_numpy_processed_X = []\n",
        "data_numpy_processed_y = []\n",
        "\n",
        "for i in data_numpy:\n",
        "  spectrogram, label = preprocess(i[0],i[1])\n",
        "  data_numpy_processed_X.append(spectrogram.numpy())\n",
        "  data_numpy_processed_y.append(label)\n",
        "\n",
        "  # spectrogram\n",
        "print(len(data_numpy_processed_X))\n",
        "\n",
        "# data = data.map(preprocess)\n",
        "# data_numpy = [[]]\n",
        "# # for i in data.take(1):\n",
        "# #   # print(i[0].numpy())\n",
        "# #   print(i[1].numpy())\n",
        "\n",
        "# # 問題のところ、データセットの次元を変えたい。（flattenしたい）\n",
        "# # data = data.flat_map(tf.data.Dataset.from_tensor_slices)\n",
        "# data = data.cache()\n",
        "# data = data.shuffle(buffer_size=1000)\n",
        "# data = data.batch(16)\n",
        "# data = data.prefetch(8)\n",
        "# print(len(data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8KzLz0OHTQT"
      },
      "source": [
        "## 6.2 Split into Training and Testing Partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZQJy_TnHTQT"
      },
      "outputs": [],
      "source": [
        "# train = data.take(36)\n",
        "# test = data.skip(36).take(15)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_X, test_X, train_y, test_y = train_test_split(data_numpy_processed_X, data_numpy_processed_y, test_size=0.2)\n",
        "train_X = np.array(train_X)\n",
        "train_y = np.array(train_y)\n",
        "test_X = np.array(test_X)\n",
        "test_y = np.array(test_y)\n",
        "\n",
        "# #　正規化\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(data)\n",
        "# newdata = scaler.transform(data)\n",
        "\n",
        "data_numpy_processed_X = np.array(data_numpy_processed_X)\n",
        "print(data_numpy_processed_X.shape)\n",
        "\n",
        "# train_X = np.array(data_numpy_processed_X)\n",
        "# train_y = np.array(data_numpy_processed_y)\n",
        "\n",
        "# # shuffle\n",
        "# print(train_X.shape)\n",
        "# indices = np.arange(train_X.shape[0])\n",
        "# np.random.shuffle(indices)\n",
        "# train_X = train_X[indices]\n",
        "# train_y = train_y[indices]\n",
        "\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(train_X.shape[1])\n",
        "\n",
        "print(np.max(train_X[0]))\n",
        "print(np.min(train_X[0]))\n",
        "# save the data\n",
        "np.save(\"train_X\",train_X)\n",
        "np.save(\"train_y\",train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データを読み込む"
      ],
      "metadata": {
        "id": "upKF-xcxfo6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #　もしデータがすでに保存されているなら、直接読み込みます\n",
        "# import os\n",
        "# if os.path.exists(\"/content/drive/MyDrive/audio_processing/intern/model1_tem_var/train_X.npy\"):\n",
        "#   train_X = np.load(\"train_X.npy\")\n",
        "#   train_y = np.load(\"train_y,npy\")"
      ],
      "metadata": {
        "id": "D4BZ3DWYfjpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF-k_lhAHTQT"
      },
      "source": [
        "# 7. Build Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZK608QqHTQT"
      },
      "source": [
        "## 7.1 Load Tensorflow Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPqjqq2LHTQT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JeYLWrbHTQT"
      },
      "source": [
        "## 7.2 Build Sequential Model, Compile and View Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdvzPyu-HTQT"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3,3), activation='relu', input_shape=(train_X.shape[1], 257,1)))\n",
        "model.add(Conv2D(16, (3,3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dense(13, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_PXekmJHTQT"
      },
      "outputs": [],
      "source": [
        "model.compile('Adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuStx59WHTQU"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syNKEmYuHTQU"
      },
      "source": [
        "## 7.3 Fit Model, View Loss and KPI Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-aSyqVmHTQU"
      },
      "outputs": [],
      "source": [
        "hist = model.fit(train_X, train_y, epochs=10, validation_split=0.2, batch_size = 32)   \n",
        "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
        "model.save(\"my_model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(hist))\n",
        "print(hist.history.keys())"
      ],
      "metadata": {
        "id": "eVFXqNeulRoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMRPKelbHTQU"
      },
      "outputs": [],
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(hist.history['loss'], 'r',label=\"train\")\n",
        "plt.plot(hist.history['val_loss'], 'b', label=\"test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxwwz4vHHTQU"
      },
      "outputs": [],
      "source": [
        "plt.title('accuracy')\n",
        "plt.plot(hist.history['accuracy'], 'r', label=\"train\")\n",
        "plt.plot(hist.history['val_accuracy'], 'b', label=\"validation\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(test_X.shape)\n",
        "result1 = model.predict(test_X)\n",
        "sum = 0\n",
        "for i in range(len(result1)):\n",
        "  if np.argmax(result1[i]) == np.argmax(test_y[i]):\n",
        "    sum += 1\n",
        "print(sum / len(test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **８.音声断片に対して分類を行う、\n"
      ],
      "metadata": {
        "id": "brtWw8RAnhrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルを直接読み込む"
      ],
      "metadata": {
        "id": "rbxPbd3tgRVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "# It can be used to reconstruct the model identically.\n",
        "model = keras.models.load_model(\"my_model\")\n",
        "if os.path.exists(\"/content/drive/MyDrive/audio_processing/intern/model1/my_model\"):\n",
        "  model = keras.models.load_model(\"my_model\")"
      ],
      "metadata": {
        "id": "vufG4R5YgQ0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#森田\n",
        "import os\n",
        "\n",
        "t01 = []\n",
        "t02 = []\n",
        "t03 = []\n",
        "t04 = []\n",
        "t05 = []\n",
        "t06 = []\n",
        "t07 = []\n",
        "t08 = []\n",
        "t09 = []\n",
        "t10 = []\n",
        "t11 = []\n",
        "t12 = []\n",
        "t13 = []\n",
        "\n",
        "ts = [t01, t02, t03, t04, t05, t06, t07, t08, t09, t10, t11, t12, t13]\n",
        "\n",
        "for n in range(13):\n",
        "    \n",
        "  for i in os.listdir(SS[n]):\n",
        "    # CALCULATE NUM\n",
        "    wav = load_wav_16k_mono(SS[n] + '/' + i)\n",
        "    # wav = wav[int(len(wav) * 0.7):]\n",
        "    \n",
        "    NUM = int(len(wav)*0.3 / (10 * 16000))\n",
        "\n",
        "    for j in range(NUM):\n",
        "      ts[n].append(os.path.join(SS[n],i+str(j)))\n",
        "\n",
        "\n",
        "  ts[n] = tf.data.Dataset.from_tensor_slices(ts[n])\n"
      ],
      "metadata": {
        "id": "a7gNnPD62gbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#森田\n",
        "# data = tf.data.Dataset()\n",
        "tis = []\n",
        "for i in range(13):\n",
        "\n",
        "  indices = i\n",
        "  depth = 13    \n",
        "\n",
        "  tis.append( tf.data.Dataset.zip((ts[i], tf.data.Dataset.from_tensor_slices(([tf.one_hot(indices, depth)]*len(ts[i]))))) )\n",
        "\n",
        "for i in range(len(tis)-1):\n",
        "  tis[0] = tis[0].concatenate(tis[i+1])\n",
        "data_test = tis[0]\n",
        "#positives = tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.ones(len(pos)))))\n",
        "#negatives = tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.zeros(len(neg)))))\n",
        "#data = positives.concatenate(negatives)\n",
        "\n",
        "print(len(data_test))\n",
        "\n",
        "print(data_test.as_numpy_iterator().next())"
      ],
      "metadata": {
        "id": "RVGXNl7r2pJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # !zip -r /content/my_model.zip /content/my_model\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/my_model.zip\")"
      ],
      "metadata": {
        "id": "3IPM--BUpinN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(file_path, label):\n",
        "    num = \"\"\n",
        "    for i in reversed(file_path):\n",
        "      if 48<=i<=57:\n",
        "        num = chr(i) + num\n",
        "      else:\n",
        "        break\n",
        "    wav = load_wav_16k_mono(file_path[:-len(num)])\n",
        "\n",
        "    num = int(num)\n",
        "    print(num)\n",
        "\n",
        "    wav_clip = wav[int(len(wav) * 0.7)+160_000*num:int(len(wav) * 0.7)+ 160_000+160_000*num]\n",
        "    length_clip_clip = int(0.75 * 16_000)\n",
        "    spectrogram_list = []\n",
        "    # print(int(len(wav) * 0.7)+ 160_000*num, int(len(wav) * 0.7)+ 160_000+160_000*num)\n",
        "    for i in range(int(len(wav_clip)/length_clip_clip)):\n",
        "      wav_clip_clip = wav_clip[length_clip_clip * i: length_clip_clip + length_clip_clip * i]\n",
        "      spectrogram = tf.signal.stft(wav_clip_clip, frame_length=320, frame_step=32)\n",
        "      spectrogram = tf.abs(spectrogram)\n",
        "      spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "      spectrogram_list.append(spectrogram.numpy())\n",
        "    return spectrogram_list, label"
      ],
      "metadata": {
        "id": "Ljpw-5KucJtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data_numpy_test = []\n",
        "for i in data_test:\n",
        "  data_numpy_test.append([i[0].numpy(), i[1].numpy()])\n",
        "\n",
        "data_numpy_processed_X_test = []\n",
        "data_numpy_processed_y_test = []\n",
        "\n",
        "for i in data_numpy_test:\n",
        "  spectrogram, label = preprocess(i[0],i[1])\n",
        "  data_numpy_processed_X_test.append(spectrogram)\n",
        "  data_numpy_processed_y_test.append(label)\n",
        "\n",
        "  # spectrogram\n",
        "\n",
        "test_X = np.array(data_numpy_processed_X_test)\n",
        "test_y = np.array(data_numpy_processed_y_test)\n",
        "\n",
        "# shuffle\n",
        "print(test_X.shape)\n",
        "indices = np.arange(test_X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "test_X = test_X[indices]\n",
        "test_y = test_y[indices]\n",
        "\n",
        "print(test_X.shape)\n",
        "print(test_y.shape)\n",
        "print(test_X.shape[1])\n",
        "\n",
        "# save the data\n",
        "np.save(\"test_X\",test_X)\n",
        "np.save(\"test_y\",test_y)\n",
        "\n"
      ],
      "metadata": {
        "id": "Uee1KxrnjFbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists(\"/content/drive/MyDrive/audio_processing/intern/model1_tem_var/test_X.npy\"):\n",
        "  test_X = np.load(\"test_X.npy\")\n",
        "  test_y = np.load(\"test_y.npy\")"
      ],
      "metadata": {
        "id": "Fm_cksndkKkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テストの精度を計算\n",
        "sum_acc = 0\n",
        "print(test_X.shape)\n",
        "print(type(test_X[1][1]))\n",
        "for index in range(len(test_X)):\n",
        "  res_pos = model.predict(test_X[index])\n",
        "  result_num = np.argmax(res_pos.sum(axis=0))\n",
        "  if result_num == np.argmax(test_y[index]):\n",
        "    sum_acc += 1\n",
        "print(\"accuracy\", sum_acc/len(test_y))"
      ],
      "metadata": {
        "id": "NLjeKRPHkeTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.音声を読み込み、テストを行う"
      ],
      "metadata": {
        "id": "dZdlzZVLm0WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/audio_processing/intern/AFPID-II/AFPID-Raw/S01/S01_1_footstep_audio_1.wav\"\n",
        "wav = load_wav_16k_mono(file_path)\n",
        "\n",
        "# spectrogramを計算する\n",
        "num = 2 # wavのどの10sの部分を切り取るか\n",
        "wav_clip = wav[int(len(wav) * 0.7)+160_000*num:int(len(wav) * 0.7)+ 160_000+160_000*num]\n",
        "length_clip_clip = int(0.75 * 16_000) # 0.75秒\n",
        "spectrogram_list = []\n",
        "# print(int(len(wav) * 0.7)+ 160_000*num, int(len(wav) * 0.7)+ 160_000+160_000*num)\n",
        "for i in range(int(len(wav_clip)/length_clip_clip)):\n",
        "  wav_clip_clip = wav_clip[length_clip_clip * i: length_clip_clip + length_clip_clip * i]\n",
        "  spectrogram = tf.signal.stft(wav_clip_clip, frame_length=320, frame_step=32)\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "  spectrogram_list.append(spectrogram.numpy())\n",
        "\n",
        "\n",
        "\n",
        "res_pos = model.predict(spectrogram_list)\n",
        "result_num = np.argmax(res_pos.sum(axis=0))\n",
        "if result_num == np.argmax(test_y[index]):\n",
        "  sum_acc += 1\n",
        "print(\"accuracy\", sum_acc/len(test_y))\n",
        "\n",
        "print(\"classification \",np.argmax(res_pos.sum(axis=0)))\n",
        "  "
      ],
      "metadata": {
        "id": "AHzOUqiFm7_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **予備**"
      ],
      "metadata": {
        "id": "015NRMQRnSJ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVSLqfhBHTQU"
      },
      "source": [
        "# 8. Make a Prediction on a Single Clip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bqnEyylHTQU"
      },
      "source": [
        "## 8.1 Get One Batch and Make a Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q39H_61dHTQU"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = test.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jotI8dHfHTQU"
      },
      "outputs": [],
      "source": [
        "yhat = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-59oPHr8HTQU"
      },
      "source": [
        "## 8.2 Convert Logits to Classes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epW7UCjpHTQU"
      },
      "outputs": [],
      "source": [
        "yhat = [1 if prediction > 0.99 else 0 for prediction in yhat]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWzfYZF_HTQV"
      },
      "source": [
        "# 9. Build Forest Parsing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_tEXDdpHTQV"
      },
      "source": [
        "## 9.1 Load up MP3s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efADRE0PHTQV"
      },
      "outputs": [],
      "source": [
        "def load_mp3_16k_mono(filename):\n",
        "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
        "    res = tfio.audio.AudioIOTensor(filename)\n",
        "    # Convert to tensor and combine channels \n",
        "    tensor = res.to_tensor()\n",
        "    tensor = tf.math.reduce_sum(tensor, axis=1) / 2 \n",
        "    # Extract sample rate and cast\n",
        "    sample_rate = res.rate\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    # Resample to 16 kHz\n",
        "    wav = tfio.audio.resample(tensor, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKo4L2ByHTQV"
      },
      "outputs": [],
      "source": [
        "mp3 = os.path.join('/content/drive/MyDrive/audio_processing/audioClassification/data', 'Forest Recordings', 'recording_00.mp3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0C12F5VHTQV"
      },
      "outputs": [],
      "source": [
        "wav = load_mp3_16k_mono(mp3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gILCTPBRHTQV"
      },
      "outputs": [],
      "source": [
        "audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=48000, sequence_stride=48000, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRWditjPHTQV"
      },
      "outputs": [],
      "source": [
        "samples, index = audio_slices.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Woslc-gQHTQV"
      },
      "source": [
        "## 9.2 Build Function to Convert Clips into Windowed Spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3nUZbs9HTQV"
      },
      "outputs": [],
      "source": [
        "def preprocess_mp3(sample, index):\n",
        "    sample = sample[0]\n",
        "    zero_padding = tf.zeros([48000] - tf.shape(sample), dtype=tf.float32)\n",
        "    wav = tf.concat([zero_padding, sample],0)\n",
        "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "    return spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QBPh4NwHTQV"
      },
      "source": [
        "## 9.3 Convert Longer Clips into Windows and Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg2K8Kz1HTQV"
      },
      "outputs": [],
      "source": [
        "audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=16000, sequence_stride=16000, batch_size=1)\n",
        "audio_slices = audio_slices.map(preprocess_mp3)\n",
        "audio_slices = audio_slices.batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UuA-FuTHTQW"
      },
      "outputs": [],
      "source": [
        "yhat = model.predict(audio_slices)\n",
        "yhat = [1 if prediction > 0.5 else 0 for prediction in yhat]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyuLlMWJHTQW"
      },
      "source": [
        "## 9.4 Group Consecutive Detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6veYtTEHTQW"
      },
      "outputs": [],
      "source": [
        "from itertools import groupby"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAFl-4l7HTQW"
      },
      "outputs": [],
      "source": [
        "yhat = [key for key, group in groupby(yhat)]\n",
        "calls = tf.math.reduce_sum(yhat).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b5pqHi2HTQW"
      },
      "outputs": [],
      "source": [
        "calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3bzXctMHTQW"
      },
      "source": [
        "# 10. Make Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdDjAcuxHTQW"
      },
      "source": [
        "## 10.1 Loop over all recordings and make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cum92eDaHTQW"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "for file in os.listdir(os.path.join('/content/drive/MyDrive/audio_processing/audioClassification/data', 'Forest Recordings')):\n",
        "    FILEPATH = os.path.join('/content/drive/MyDrive/audio_processing/audioClassification/data','Forest Recordings', file)\n",
        "    \n",
        "    wav = load_mp3_16k_mono(FILEPATH)\n",
        "    audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=48000, sequence_stride=48000, batch_size=1)\n",
        "    audio_slices = audio_slices.map(preprocess_mp3)\n",
        "    audio_slices = audio_slices.batch(64)\n",
        "    \n",
        "    yhat = model.predict(audio_slices)\n",
        "    \n",
        "    results[file] = yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kYc3nghHTQW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYA4FZGVHTQW"
      },
      "source": [
        "## 10.2 Convert Predictions into Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7AHqAtFHTQW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class_preds = {}\n",
        "for file, logits in results.items():\n",
        "    class_preds[file] = [1 if prediction > 0.99 else 0 for prediction in logits]\n",
        "class_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnIzERuLHTQX"
      },
      "source": [
        "## 10.3 Group Consecutive Detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIkR643cHTQX",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "postprocessed = {}\n",
        "for file, scores in class_preds.items():\n",
        "    postprocessed[file] = tf.math.reduce_sum([key for key, group in groupby(scores)]).numpy()\n",
        "postprocessed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVeVa3kUHTQX"
      },
      "source": [
        "# 11. Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXzYfsAOHTQX"
      },
      "outputs": [],
      "source": [
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5ODzjxDHTQX"
      },
      "outputs": [],
      "source": [
        "with open('results.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter=',')\n",
        "    writer.writerow(['recording', 'capuchin_calls'])\n",
        "    for key, value in postprocessed.items():\n",
        "        writer.writerow([key, value])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltkcMDEaeiJ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpKTFOSLgN3Z"
      },
      "source": [
        "劉　問題コード\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es2pXm4LgNlX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx47HKxkgQ7t"
      },
      "source": [
        "## 5.1 Build Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKTHntUbgQ7t"
      },
      "outputs": [],
      "source": [
        "def preprocess(file_path, label): \n",
        "    wav = load_wav_16k_mono(file_path)\n",
        "    wav = wav[:48000]\n",
        "    zero_padding = tf.zeros([48000] - tf.shape(wav), dtype=tf.float32)\n",
        "    wav = tf.concat([zero_padding, wav],0)\n",
        "    # audio_slices = \n",
        "    # audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=11200, sequence_stride=11200, batch_size=1)\n",
        "    result_list = []\n",
        "    wav_list = [wav[12000*i:12000+12000*i] for i in range(4)]\n",
        "    for wav_single in wav_list:\n",
        "      spectrogram = tf.signal.stft(wav_single, frame_length=320, frame_step=32)\n",
        "      spectrogram = tf.abs(spectrogram)\n",
        "      spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "      result_list.append([spectrogram, label])\n",
        "    return result_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kFuZUvZgU4_"
      },
      "source": [
        "## 6.1 Create a Tensorflow Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEkotXPVgU5A"
      },
      "outputs": [],
      "source": [
        "data = data.map(preprocess)\n",
        "data = data.flat_map(tf.data.Dataset.from_tensor_slices)\n",
        "data = data.cache()\n",
        "data = data.shuffle(buffer_size=1000)\n",
        "data = data.batch(16)\n",
        "data = data.prefetch(8)\n",
        "print(len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2xAvQSIoIkx"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWBfXQ6uwesY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iplbsjxwUXJ3"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "def save_model(model):\n",
        "  # モデルの保存\n",
        "  filename = 'finalized_model.sav'\n",
        "  pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "def load_model(filename):\n",
        "  # モデルを読み込み\n",
        "  loaded_model = pickle.load(open(filename, 'rb'))\n",
        "  return loaded_model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HVSLqfhBHTQU",
        "JWzfYZF_HTQV",
        "E3bzXctMHTQW",
        "fVeVa3kUHTQX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}